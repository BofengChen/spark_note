{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# windows 系统下spark 安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装 java\n",
    "\n",
    "step1 去官网主页下载 [JDK](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html),版本选择jdk-8u201-windows-x64.exe；\n",
    "\n",
    "step2 下载后JDK的安装根据提示进行，安装JDK的时候也会安装JRE，一并安装就可以。安装过程中可以自定义安装目录等信息；我的目录是：`C:\\Program Files\\Java\\jdk1.8.0_201`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step3 配置环境变量：\n",
    "* 添加 JAVA_HOME 路径\n",
    "* 添加 PATH 路径\n",
    "* 添加 CLASSPATH 路径\n",
    "\n",
    "安装完成后，右击\"我的电脑\"，点击\"属性\"，选择\"高级系统设置\"；\n",
    "\n",
    "<img src='figure/java0.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择\"高级\"选项卡，点击\"环境变量\",出现如下界面；\n",
    "\n",
    "\n",
    "<img src='figure/java1.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 添加 JAVA_HOME 路径\n",
    "\n",
    "选择“新建”：输入`JAVA_HOME` 和对应安装路径 `C:\\Program Files\\Java\\jdk1.8.0_201`\n",
    "\n",
    "<img src='figure/java2.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 添加 PATH 路径\n",
    "\n",
    "点击`PATH`，选择编辑;之后选择‘新建’。\n",
    "\n",
    "<img src='figure/java3.png' width='400' height='300' align='left'>\n",
    "<img src='figure/java4.png' width='400' height='300' align='right'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点击‘新建’，添加`C:\\Program Files\\Java\\jdk1.8.0_201\\bin`：\n",
    "\n",
    "<img src='figure/java5.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 添加 CLASSPATH 路径：\n",
    "\n",
    "点击‘新建’，添加 `C:\\Program Files\\Java\\jdk1.8.0_201\\lib`\n",
    "\n",
    "<img src='figure/java6.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 打开`cmd`，输入`java -version`,`javac`如果出现如下界面说明安装成功：\n",
    "\n",
    "<img src='figure/java7.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装 Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark的各个版本需要跟相应的Scala版本对应,目前最新的Spark 2.0就只能使用Scala 2.11的各个版本，所以下载的时候需要注意到这种Scala版本与Spark版本相互对应的关系。\n",
    "\n",
    "我安装的版本是：[scala-2.11.8.msi](https://www.scala-lang.org/download/2.11.8.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接点击安装即可，安装结束后将bin路径`C:\\Program Files (x86)\\scala\\bin`添加到环境变量的`PATH`即可。\n",
    "\n",
    "安装结束后，打开 `cmd`输入 `scala`出现如下界面表示安装成功：\n",
    "\n",
    "<img src='figure/scala0.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装 Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装基于 Hadoop2.7之上的[spark-2.3.2](https://archive.apache.org/dist/spark/spark-2.3.2/)\n",
    "\n",
    "注意：在Win10上安装 spark 2.3.3以上的版本后，启动`pyspark`之后会出现`ImportError: No module named 'resource'`错误，所以最终选择了2.3.2版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/spark0.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将安装包解压到根目录，并且可以重命名，我命名为`spark232_hadoop27`：\n",
    "\n",
    "* 将 SPARK_HOME 设置为 `C:\\spark232_hadoop27`;\n",
    "\n",
    "* 将bin的路径`C:\\spark232_hadoop27\\bin`添加到环境变量 `PATH`\n",
    "\n",
    "打开`cmd`输入`spark-shell`出现如下界面就说明安装成功：\n",
    "\n",
    "<img src='figure/spark1.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入`pyspark`就可以进入python版本接口的spark\n",
    "\n",
    "<img src='figure/spark2.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装 Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载 Hadoop 版本，要和安装 Spark 所需求的版本一致，我选择的是 [hadoop-2.7.7.tar](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz)\n",
    "\n",
    "<img src='figure/hadoop0.png' width='500' height='400' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 下载之后解压到本地，可以对进行重命名，我的文件位置是`C:\\hadoop277`;\n",
    "* 配置HADOOP_HOME环境变量，`HADOOP_HOME`设置为 `C:\\hadoop277`;\n",
    "* 配置PATH环境变量，新建 `C:\\hadoop277\\bin`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题1**：`Please update C:\\hadoop277\\conf\\hadoop-env.cmd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决办法：在路径`C:\\hadoop277\\etc\\hadoop`下找到 `hadoop-env`文件，打开编辑：将`set JAVA_HOME=%JAVA_HOME%`注释，改成`set JAVA_HOME=C:\\PROGRA~1\\Java\\jdk1.8.0_201`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题2**：本地运行spark程序缺少hadoop winutils.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载zip文件 [hadoop-common-2.2.0-bin-master](https://github.com/srccodes/hadoop-common-2.2.0-bin)到本地进行解压，将里面的`winutils.exe`文件复制到`C:\\hadoop277\\bin`下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jupyter 中使用pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T17:42:12.598690Z",
     "start_time": "2019-05-13T17:42:12.583689Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "# 找到spark的安装目录\n",
    "spark_path = 'C:\\spark232_hadoop27'\n",
    "findspark.init(spark_path,edit_rc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T17:42:13.173703Z",
     "start_time": "2019-05-13T17:42:13.161203Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T17:54:48.227798Z",
     "start_time": "2019-05-13T17:54:48.200298Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T17:56:56.383758Z",
     "start_time": "2019-05-13T17:56:56.366258Z"
    }
   },
   "outputs": [],
   "source": [
    "conf1 = pyspark.SparkConf().setAll([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T17:56:57.268273Z",
     "start_time": "2019-05-13T17:56:56.856267Z"
    }
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-ed69b356bc11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHiveContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark232_hadoop27\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark232_hadoop27\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark232_hadoop27\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:\\spark232_hadoop27\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark232_hadoop27\\python\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=conf1).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc1 = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-13T17:30:25.740483Z",
     "start_time": "2019-05-13T17:30:25.720483Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fad25e8464e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\README.md'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(spark_path + '\\README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count is 103;first str is # Apache Spark\n"
     ]
    }
   ],
   "source": [
    "print('count is {};first str is {}'.format(lines.count(),lines.first()))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 访问 Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 Spark服务启动后，可以在[这里](http://localhost:4040/jobs/)看到 Spark 相关的运行信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如执行`sc.textFile(spark_path + '\\README.md').count()`的操作，就会看到`Jobs`会有相应的记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/sparkUI0.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 简介与入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MapReduce 中文版](http://blog.bizcloudsoft.com/wp-content/uploads/Google-MapReduce%E4%B8%AD%E6%96%87%E7%89%88_1.0.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark 是一个用来实现快速而通用的集群计算平台：\n",
    "\n",
    "* 速度方面，Spark 扩展了广泛使用的 MapReduce 计算模型，而且更高效地支持了更多计算模式，包括交互式查询和流处理；Spark 的一个主要特点是可以在内存中进行计算，所以更快；\n",
    "\n",
    "\n",
    "* Spark 在一个统一的框架下支持不同场景的计算，包括批处理、迭代算法、交互式查询、流处理；\n",
    "\n",
    "\n",
    "* Spark 提供的接口十分丰富，提供了基于 Python、Java、Scala 和 SQL 这些简单易用的API，Spark 还可以和其他大数据工具密切配合使用，比如 Spark 可以运行在 Hadoop 集群上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark 的各个组件介绍：**\n",
    "\n",
    "* **Spark Core:** 实现了 Spark 的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块，Spark Core 还包含了对 _**弹性分布式数据集(resilient distributed dataset，简称 RDD)**_ 的 API 定义。 之后专门会有一章来介绍RDD；\n",
    "\n",
    "\n",
    "* **Spark SQL:** 是 Spark 用来操作结构化数据的程序包。通过 SQL 或者 Apahce Hive 版本的 SQL 方言(HQL) 来查询数据。Spark SQL 支持多种数据集，比如 Hive 表、Parquet 以及 JSON 等。Spark SQL 还支持开发者和传统的 RDD 编程的数据操作方式相结合；\n",
    "\n",
    "\n",
    "* **Spark Streaming:** 是 Spark 提供的对实时数据进行流式计算的组件，比如网络服务中用户提交的状态更新组成的消息队列都是数据流；Spark Streaming 天给了操作数据流的 API，并且可以和 Spark Core 中的 RDD API 高度对应；\n",
    "\n",
    "\n",
    "* **Spark MLib:** 是 Spark 所包含的一个提供常见的机器学习功能的程序库，提供了常见的机器学习算法，还提供了模型评估、数据导入等功能。 MLib 还提供了一些更底层的机器学习原语，包括一个通用的梯度优化算法；\n",
    "\n",
    "\n",
    "* **Spark GraphX**：是 Spark 提供用来操作图(比如社交网络的朋友关系图)的程序库，可以进行并行的图计算。它扩展了 Spark 的 RDD API，可以用来创建一个顶点和边都包含任意属性的有向图，还支持针对图的各种操作以及一些常用的图算法；\n",
    "\n",
    "\n",
    "* **集群管理器**： 从底层角度考虑，Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算，为了实现这样的要求，同时获得最大灵活性，Spark 支持各种 _**集群管理器**_ 上运行，包括 Hadoop YARN、Apache Mesos 以及 Spark 自带的简易调度器——独立调度器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/spark-11.png' width='400' height='300' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark 核心概念简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark 应用都由 一个**驱动器程序(driver program)** 来发起集群上的各种并行操作。驱动器程序包含了应用的主要 main 函数，并且定义了集群上的分布式数据集，还对这些分布式数据集应用了相关操作。\n",
    "\n",
    "驱动器程序通过一个 SparkContext 对象来访问 Spark，这个对象代表计算集群的一个连接。 shell 启动时候已经自动创建了一个 SparkContext 对象，是一个叫 sc 的变量。一旦有了 SparkContext,你就可以创建 RDD，并且对 RDD 进行各种操作。\n",
    "\n",
    "要执行这些操作， driver program 一般要管理多个 **执行器(executor)节点**。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figure/driver0.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外，我们有很多传递函数的 Spark API，可以将对应操作运行在集群上。Spark API 最神奇的地方在于，像 `filter` 这样基于函数的操作也会在集群上并行执行。也就是说，Spark 会自动将函数发到各个 executor 节点上。 所以你可以在单一的驱动器程序上编程，并且让代码自动运行在多个节点上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python 中运用 Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了交互式运行之外，Spark 也可以在 Java、Scala 或 Python 的独立程序中被连接使用。**这与在 shell 中使用的主要区别在于你需要自行初始化 SparkContext。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们主要介绍在python 中如何使用。在 Python 中，你可以把应用写成 Python 脚本，但是需要使用 Spark 自带的 `bin/spark-submit` 脚本来运行。spark-submit 脚本会帮我们引入 Python 程序的 Spark 依赖，为 Spark 的 PythonAPI 配置好了运行环境。只需要如下这样运行脚本就可以：\n",
    "\n",
    "`bin/spark-submit my_script.py`\n",
    "\n",
    "windows 中需要使用反斜杠来代替斜杠。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**初始化 SparkContext**\n",
    "\n",
    "在Python 程序中可以通过导入 Spark 包来创建 SparkContext。可以先创建一个 SparkConf 来配置你的应用，基于这个 SparkConf 创建一个 SparkContext 对象。下面是一个举例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# 找到spark的安装目录\n",
    "spark_path = 'C:\\spark232_hadoop27'\n",
    "findspark.init(spark_path,edit_rc=True)\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster('local').setAppName('Chen_App')\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "     print('there is none sparkcntext on running')   \n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-ISDP5K2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Chen_App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Chen_App>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子展示了创建 SparkContext 最基本的方法，你只需要传递两个参数给 SparkConf:\n",
    "\n",
    "* 集群 URL:告诉 Spark 如何连接到集群上，这个例子中我们使用的是 `local`，可以让Spark 运行在单机单线程上而无需连接到集群；\n",
    "* 应用名：例子里我们使用的是 `Chen_App`,应用名可以帮助你在集群管理器的用户界面中找到你的应用。\n",
    "\n",
    "还有很多附加参数可以用来配置应用的运行方式，我们之后会介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关闭 Spark 可以调用 SparkContext 的 `stop()` 方法，或者直接退出应用(比如 `System.exit(0)` or `sys.exit()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "     print('there is none sparkcntext on running')   "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
