{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark 编程进阶**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章介绍两种类型的共享变量：**累加器(accumulator)** 与 **广播变量(broadcast variable)**。\n",
    "\n",
    "累加器用来对信息进行聚合，广播变量用来高效分发比较大的对象。\n",
    "\n",
    "通常在向 Spark 传递函数时，比如使用 `map()` 或者 `filter()` 传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新副本的值不会改变驱动器中的对应变量。Spark 的两个共享变量——**累加器与广播变量**——分别为结果聚合与广播这两种常见的通信模式突破这一限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# 找到spark的安装目录\n",
    "spark_path = 'C:\\spark232_hadoop27'\n",
    "findspark.init(spark_path,edit_rc=True)\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('Chen_App')\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "     print('there is no sparkcntext on running')   \n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 累加器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark 的第一种共享变量就是**累加器**，累加器提供了将工作节点中的值聚合到驱动器程序中的简单语法。累加器的一个常见用途在调试时对作业执行过程中的事件进行**计数**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class pyspark.Accumulator()\n",
    "\n",
    "`class pyspark.Accumulator(aid, value, accum_param)`:\n",
    "\n",
    "A shared variable that can be accumulated, i.e., has a commutative and associative “add” operation. Worker tasks on a Spark cluster can add values to an Accumulator with the **+=** operator, but only the driver program is allowed to access its **value**, using value. Updates from the workers get propagated automatically to the driver program.\n",
    "\n",
    "While **SparkContext** supports accumulators for primitive data types like **int** and **float**, users can also define accumulators for custom types by providing a custom **AccumulatorParam** object.Refer to the doctest of this module for an example.\n",
    "\n",
    "* `add(term)`:Adds a term to this accumulator’s value\n",
    "\n",
    "* `value`:Get the accumulator’s value; only usable in driver program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：工作节点上的任务不能访问累加器的值。从这些任务角度看，**累加器是一个只写变量。**\n",
    "\n",
    "在这种模式下，累加器的实现可以更加高效，不需要对每次更新操作进行复杂的通信。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accumulator()\n",
    "\n",
    "`accumulator(value, accum_param=None)`:\n",
    "\n",
    "Create an **Accumulator** with the given initial value, using a given **AccumulatorParam** helper object to define how to add values of the data type if provided. Default AccumulatorParams are used for **integers and floating-point** numbers if you do not provide one. For other types, a custom AccumulatorParam can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：在刚开始启动 SparkContext 时，`accumulator()` 属于 `class pyspark.SparkContext()` 下面的一个方法，所以不用单独导入`from pyspark import Accumulator` 就可以使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after add op value is:9\n",
      "after += op value is:10\n"
     ]
    }
   ],
   "source": [
    "acc=sc.accumulator(0)\n",
    "acc.add(9)\n",
    "print('after add op value is:{}'.format(acc.value))\n",
    "acc += 1\n",
    "print('after += op value is:{}'.format(acc.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class pyspark.AccumulatorParam\n",
    "\n",
    "`class pyspark.AccumulatorParam\n",
    "`:Helper object that defines how to accumulate values of a given type.\n",
    "\n",
    "* `addInPlace(value1, value2)`:Add two values of the accumulator’s data type, returning a new value; for efficiency, can also update value1 in place and return it.\n",
    "\n",
    "* `zero(value)`:Provide a “zero value” for the type, compatible in dimensions with the provided value (e.g., a zero vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chenbofeng_abc', 6)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import AccumulatorParam\n",
    "\n",
    "# 自定义一个累加器，对tuple 的每一项进行求和\n",
    "class StringAndIntAccumulator(AccumulatorParam):\n",
    "    def zero(self, s):\n",
    "        return (s[0]+'chenbofeng_',0)\n",
    "    def addInPlace(self, s1, s2):\n",
    "        return (s1[0]+s2[0], s1[1]+s2[1])\n",
    "\n",
    "accumulator = sc.accumulator((\"\",0), StringAndIntAccumulator())\n",
    "\n",
    "def add(x): \n",
    "    global accumulator\n",
    "    accumulator += x\n",
    "\n",
    "sc.parallelize([(\"a\",1), (\"b\",2), (\"c\",3)]).foreach(add)\n",
    "\n",
    "accumulator.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面这个例子中，`accumulator` 有自己的两个属性`add`以及`value`，但是`StringAndIntAccumulator`继承了类`AccumulatorParam`的两个方法`zero`和`addInPlace`，重新定义了 `add` 和 `zero`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广播变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark 的第一种共享变量就是**广播变量**，它可以让程序高效地向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如如果应用需要向所有节点发送一个较大的只读查询表或者机器学习算法中的一个很大的特征向量，广播变量都会是很好地选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用广播变量的方式很简单：\n",
    "\n",
    "* 通过一个类型 T 的对象调用 `SparkContext.broadcast` 创建一个 `Broadcast[T]` 对象。任何序列化的类型都可以这么实现；\n",
    "\n",
    "\n",
    "* 通过 `value` 属性访问该对象的值；\n",
    "\n",
    "\n",
    "* 变量只会被发到各个节点一次，应作为**只读值**处理，修改这个值不会影响到别的节点。\n",
    "\n",
    "    满足**只读**要求最容易的方式是广播基本类型的值或者引用不可变对象；但有时候传输一个可变对象可能更为方便高效，如果这样的话，需要自己维护只读的条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class pyspark.Broadcast()\n",
    "\n",
    "`class pyspark.Broadcast(sc=None, value=None, pickle_registry=None, path=None, sock_file=None)`:A broadcast variable created with `SparkContext.broadcast()`. Access its value through `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = sc.broadcast([1, 2, 3, 4, 5])\n",
    "b.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### broadcast()\n",
    "\n",
    "`broadcast(value)`:Broadcast a **read-only variable** to the cluster, returning a `L{Broadcast<pyspark.broadcast.Broadcast>}` object for reading it in distributed functions. The variable will be sent to each cluster only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于分区进行操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。诸如打开数据库连接或创建随机数生成器等操作，都是我们应当尽量避免为每个元素都配置一次的工作。\n",
    "\n",
    "Spark 提供基于分区的 `map` 和 `foreach`，让你的部分代码只对 RDD 的每个分区运行一次，这样可以帮助降低这些操作的代价。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitions()\n",
    "\n",
    "`mapPartitions(f, preservesPartitioning=False)`:Return a new RDD by applying a function to each partition of this RDD.\n",
    "\n",
    "* 该函数和map函数类似，只不过映射函数的参数由RDD中的每一个元素变成了RDD中每一个分区的迭代器。如果在映射的过程中需要频繁创建额外的对象，使用`mapPartitions`要比`map`高效；\n",
    "\n",
    "    比如，将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个connection，这样开销很大，如果使用`mapPartitions`，那么只需要针对每一个分区建立一个connection。\n",
    "\n",
    "\n",
    "* 参数`preservesPartitioning`表示是否保留父RDD的partitioner分区信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "def f(iterator): yield sum(iterator)\n",
    "rdd.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitionsWithIndex()\n",
    "\n",
    "`mapPartitionsWithIndex(f, preservesPartitioning=False)`:Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition.\n",
    "\n",
    "函数作用同mapPartitions，不过`f`提供了两个参数，第一个参数为分区的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4], 3)\n",
    "def f(splitIndex, iterator): yield splitIndex\n",
    "rdd.mapPartitionsWithIndex(f).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述 rdd 三个分区的index为 0、1、2，所以求和为 3。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreachPartition()\n",
    "\n",
    "`foreachPartition(f)`:Applies a function to each partition of this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(iterator):\n",
    "    for x in iterator:\n",
    "        x+1\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
